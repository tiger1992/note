
相关资料链接
http://www.open-open.com/lib/view/open1417612091323.html

配置环境变量：HADOOP_HOME   (路劲为解压的插件路劲D:\hadoop-common-2.6.0)
在C:\Windows\System32\drivers\etc 的host文件上添加	172.16.88.102    hadoop1



========= HBase 主要作用 =========大数据

1、海量苏剧存储和海量数据的准实时查询

========= HBase 应用场景 =========

1、交通、金融、电商、移动

========= HBase 特点 =========

1、海量：HBase单表可以有百亿行，百万列，数据矩阵横向和纵向两个维度所支持的数据量级别都非常具有弹性。对比关系型数据库，单边不会超过500万行，列不会超过30列

2、面向列：HBase是面向列的存储和权限控制（关系型数据库是面向行的数据，需要定义表的列结构），并支持独立检索。列是可以动态增加，不需要定义列

3、多版本：每一个列的数据存储多个Version。

4、稀疏性：为空的列并不占用存储空间，表可以设计地非常稀疏。

5、扩展性：底层依赖HDFS

6、高可用性：WAL机制保证了数据写入时不会因为集群异常而导致写入数据丢失：Replication机制保证了在集群出现严重问题时数据不会发生丢失或损坏。而且HBase底层使用HDFS，HDFS本身也有备份。

7、高性能：底层LSM数据结构和RowKey有序排列等架构上的独特设计，使得HBase具有非常高的写入性能。region切分，主键索引和缓存机制使得HBase在海量数据下具备一定的随机读写性能，该性能针对RowKey的查询能够达到毫秒级别。

========= HBase 版本的选择 =========

1、官方版本：http://archive.apache.org/dist/hbase/

2、CDH版本：http://archive.cloudera.com/cdh5/对技术框架的整合，稳定，兼容性好，现实应用中更多的采用这个版本

========= HBase 表结构模型 =========

1、举例：求职简历表结构
	 个人信息(column family列簇)			教育经历		    工作经历
  姓名(column列) 年龄 电话 住址 性别     大学 研究生 博士   工作1 工作2 工作3

2、一张表列簇不会超过5个，一般是建表的时候就确定，列可以动态增加列簇不可以

3、列动态增加、数据自动划分、高并发读写、不支持条件查询

========= HBase 安装 =========

1、jdk 1.7以上

2、Hadoop-2.5.0以上

3、Zookeeper-3.4.5

========= HBase Shell ========= 

	1、启动HBase【 bin/start-hbase.sh】 

	2、停用HBase【bin/stop-hbase.sh】

	3、进入HBase客户端命令操作界面【/opt/hbase-1.2.7/bin/hbase shell】

	4、退出HBase客户端命令操作界面【quit】

	5、查看帮助命令【help】

1、通用命令

	1)、查看HBase的状态，例如服务器的数量【status】
	
	2)、HBase版本【version】	

	3)、表引用命令提供帮助【table_help】

	4)、有关用户的信息【whoami】
	
2、数据定义语言

	1)、创建表【 create 'table_name','column_family1','column_family2','xxx' 】，指定 表名 和 列族
	
	2)、查看当前数据库中有哪些表【list】
	
	3)、禁用表【 disable 'table_name'】
	
	4)、删除表【 drop 'table_name'】，只有成功禁用表之后才能删除表
	
	8)、清空表数据【 truncate 'table_name'】
	
	6)、验证表是否被禁用【is_disabled 'table_name'】
	
	7)、启用一个表【 enable 'table_name'】
	
	8)、验证表是否已启用【is_enabled 'table_name'】
	
	9)、查看表结构【 desc 'table_name'】
	
	10)、改变一个表【 alter 】
	
	11)、验证表是否存在【 exists 'table_name'】
	
	12)、丢弃在命令中给出匹配“regex”的表【 drop_all】
	
3、数据操纵语言

	1)、put: 添加/更新指定字段的数据【 put 'table_name','RowKey','column_family1:col','colValue'】
	
	2)、扫描表all数据【 scan 'table_name'】
	
	3)、查看某个RowKey范围内的数据【 scan 'table_name’',{STARTROW => 'startRowKey',STOPROW => 'stopRowKey'}】
	
	4)、查看指定行的数据【 get 'table_name','RowKey'】
	
	5)、查看指定行指定列或列族的数据【 get 'table_name','RowKey','column_family1:col'】
	
	6)、删除掉某个RowKey中某一列的数据【 delete 'table_name','RowKey','column_family1:col'】
		
	7)、删除某一个RowKey全部的数据【 deleteall 'table_name','RowKey'】
	
	8)、清空表数据【 truncate 'table_name'】

	9)、统计一张表有多少行数据【 count 'table_name'】
	
4、HBase的查询实现只提供两种方式：

	1)、按指定RowKey获取唯一一条记录，get方法（org.apache.hadoop.hbase.client.Get）

	2)、按指定的条件获取一批记录，scan方法（org.apache.hadoop.hbase.client.Scan）

5、实现条件查询功能使用的就是scan方式，scan在使用时有以下几点值得注意：

	1)、scan可以通过setCaching与setBatch方法提高速度（以空间换时间）；
	2)、scan可以通过setStartRow与setEndRow来限定范围。范围越小，性能越高。通过巧妙的RowKey设计使我们批量获取记录集合中的元素挨在一起（应该在同一个Region下），可以在遍历结果时获得很好的性能。
	
	3)、scan可以通过setFilter方法添加过滤器，这也是分页、多条件查询的基础。

6、行键设计规则：

	1)、写入要分散，减少region的热点问题。
	2)、列族尽量少
	
	
7、HBase读流程：region元信息本质上市存在RegionServer上，而HRegionServer的元信息存在zookeeper上

	1)、首先Client先去访问zookeeper，从zookeeper里面获取RegionServer的meta信息
	
	2)、接着Client通过上一步得到的HRegionServer的IP来访问meta表所在的HRegionServer，得到region meta
	
	3)、Client通过region meta，访问对应的HRegionServer，然后扫描所在HReginServer的memstore和storefile来查询数据
	
	4)、最后HRegionServer把查询到的数据响应给HBaseClient
	
7、HBase写流程：

	1)、Client先访问zookeeper，找到meta表，取得meta元数据
	
	2)、确定所需要写入数据对应得region和RegionServer服务器
	
	3)、HBaseClient向该RegionServr服务器发起写入请求，RegionServer收到并做出响应

	总：Client 访问 zookeeper，zookeeper 返回 meta(HMaster和RegionServer)，访问具体 RegionServer，将数据操作写到 Hlog(本地)，Hlog写成功后，往 MemeStore(64M/40%时写) 中写，同时往 DataNode(一个个HFile)中写数据，将小的文件合并为大文件，当合并的文件所对应的region达到了256M默认值的阈值，开始拆分region
	
8、HMaster的职责：(尽量开高可用)

	1)、负责RegionServer的负载均衡，调度RegionServer的工作
	
	2)、通过调度RegionServer去执行分割region的任务
	
10.9、查看页面
启动成功后，可以通过主机名:60010地址来访问HBase的管理页面
例如，http://hadoop-senior01.itguigu.com:60010
	
9、大数据中hdfs常用的两种数据格式【tsv格式的文件：字段之间以制表符\t分割，csv格式的文件：字段之间以逗号,分割】

10、查看HBase执行MapReduce所依赖的Jar包【/bin/hbase mapredcp】

11、运行官方的MapReduce任务【/opt/modules/cdh/hadoop-2.5.0-cdh5.3.6/bin/yarn jar lib/hbase-server-0.98.6-cdh5.3.6.jar rowcounter student】

12、在hdfs上创建文件夹【hdfs dfs -mkdir /input_fruit/】

13、在HDFS中创建xx.tsv文件夹并上传到hdfs指定文件夹【hdfs dfs -put fruit.tsv /input_fruit/】

14、执行MapReduce将xx.tsv结构化数据导入到HBase的fruit表中【importtsv -Dimporttsv.columns=HBASE_ROW_KEY,info:name,info:color fruit \
hdfs://hadoop-senior01.itguigu.com:8020/input_fruit
】

15、

	
	 sqoop import --connect jdbc:mysql://172.16.104.94:3306/fs_mobilemap  --username fs_dev --password fs_dev --table o_sense_d --hbase-table sense --column-family id


