
====== pod ======

1、体验Pod，定义pod.yml文件，比如pod_nginx_rs.yaml
cat > pod_nginx_rs.yaml <<EOF
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx
  labels:
    tier: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      name: nginx
      labels:
        tier: frontend
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
EOF

2、根据pod_nginx_rs.yml文件创建pod：【kubectl apply -f pod_nginx_rs.yaml】

3、pod的相关命令操作
  【kubectl get pods】
  【kubectl get pods -o wide】，可以到对应节点查看验证：【docker ps | grep nginx】
  【kubectl describe pod nginx】，查看pod详情信息
  【kubectl delete -f pod_nginx_rs.yaml】，删除所有pod
  【kubectl delete pod podid】， 删除一个pod
  
3、扩缩容：【kubectl scale rs nginx --replicas=5】
  【kubectl scale deployment whoami-deployment --replicas=5】
	

====== ReplicationController与ReplicaSet，升级版 ======

1、定义了一个期望的场景，即声明某种Pod的副本数量在任意时刻都符合某个预期值，所以RC的定义包含以下几个部分：

  - Pod期待的副本数（replicas）
  - 用于筛选目标Pod的Label Selector
  - 当Pod的副本数量小于预期数量时，用于创建新Pod的Pod模板（template）
  
  也就是说通过RC实现了集群中Pod的高可用，减少了传统IT环境中手工运维的工作。
  
  
====== Deployment ======

Deployment相对RC最大的一个升级就是我们可以随时知道当前Pod“部署”的进度。管理  ReplicaSet(管理pod)/ReplicationController(逐渐被淘汰) 和 pod 等资源

1、根据 nginx-deployment.yaml 文件创建pod：【kubectl apply -f nginx-deployment.yaml】

apiVersion: apps/v1
kind: Deployment
metadata: 
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:             # 匹配具有同一个label属性的pod标签
    matchLabels:
      app: nginx         
  template:             # 定义pod的模板
    metadata:
      labels:
        app: nginx      # 定义当前pod的label属性，app为key，value为nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80

2、查看 deployment 资源详情:【kubectl get deployment -o wide】

3、查看 deployment 管理的 rs 的数量；【kubectl get rs】

4、更新镜像(滚动更新)：【kubectl set image deployment nginx-deployment nginx=nginx:1.9.1】
  

====== Namespace 命名空间 ======

1、基本概念：命名空间就是为了隔离不同的资源，比如：Pod、Service、Deployment等。可以在输入命令的时候指定命名空间`-n`，如果不指定，则使用默认的命名空间：default。

2、创建命名空间：

  1)、创建yaml文件：【vi myns-namespace.yaml】,j将下面信息保存入文件中
  【
apiVersion: v1
kind: Namespace
metadata:
  name: myns
  】  
  
  2)、执行创建命令：【kubectl apply -f myns-namespace.yaml】
  
  3)、查看命名空间：【kubectl get namespaces/ns】
  
3、指定命名空间下的资源，比如创建一个pod，属于myns命名空间下
  
  1)、vi nginx-pod.yaml
  【
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
  namespace: myns
spec:
  containers:
  - name: nginx-container
    image: nginx
    ports:
    - containerPort: 80
  】
  
  2)、【kubectl apply -f nginx-pod.yaml】
  

4、查看myns命名空间下的Pod和资源：
  【kubectl get pods】，这是获取默认命名空间下资源
  【kubectl get pods -n myns】，获取指定命名空间下资源
  【kubectl get all -n myns】
  【kubectl get pods --all-namespaces】    #查找所有命名空间下的pod


====== Network 与 Service ======

1、同一个Pod中的容器通信

  1)、官网原话：Each Pod is assigned a unique IP address. Every container in a Pod shares the network namespace, including the IP address and network ports. 大概意思是说同一个pod中的容器是共享网络ip地址和端口号的
  
  2)、那如果是通过容器的名称进行通信呢？就需要将所有pod中的容器加入到同一个容器的网络中，我们把该容器称作为pod中的pause container。

2、集群内Pod之间的通信，K8S最小的操作单元，Pod之间的通信，称之为 Cluster Network

  1)、同一节点中的Pod之间的通信
  
  2)、不同节点之间pod的通信，在任意一个节点访问pod，都是建立在calico的功劳之上,，但只限于集群内进行使用
  【curl 192.168.188.208】执行命令验证能访问成功，能拼通【ping 192.168.188.208】
  

3、实验：准备两个pod，一个nginx，一个busybox

  1)、nginx_pod.yaml
  【
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
  labels:
    app: nginx
spec:
  containers:
    - name: nginx-container
      image: nginx
      ports:
      - containerPort: 80
  】

  2)、busybox_pod.yaml
  【
apiVersion: v1
kind: Pod
metadata:
  name: busybox
  labels:
    app: busybox
spec:
  containers:
    - name: busybox
      image: busybox
      command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  】

  3)、将两个pod运行起来，并且查看运行情况
  【kubectl apply -f nginx_pod.yaml】
  【kubectl apply -f busy_pod.yaml】
  【kubectl get pods -o wide】，容器运行详情如下：
  NAME        READY   STATUS    RESTARTS   AGE    IP                NODE                  NOMINATED NODE   READINESS GATES
  busybox     1/1     Running   0          36s    192.168.116.14    worker2-kubeadm-k8s   <none>           <none>
  nginx-pod   1/1     Running   0          113s   192.168.188.208   worker1-kubeadm-k8s   <none>           <none>

4、集群内相互访问 Service-ClusterIP，Service的一种类型

  1)、创建 whoami-deployment.yaml 文件，并且【kubectl apply -f whoami-deployment.yaml】
  【
apiVersion: apps/v1
kind: Deployment
metadata:
  name: whoami-deployment
  labels:
    app: whoami
spec:
  replicas: 3
  selector:
    matchLabels:
      app: whoami
  template:
    metadata:
      labels:
        app: whoami
    spec:
      containers:
      - name: whoami
        image: jwilder/whoami
        ports:
        - containerPort: 8000
  】
  
  2)、查看pod以及service【kubectl get pods -o wide】，【kubect get svc】，此时发现还没有whoami的service
  NAME                                 READY   STATUS    RESTARTS   AGE    IP                NODE                  NOMINATED NODE   READINESS GATES
  whoami-deployment-678b64444d-522q9   1/1     Running   0          5m7s   192.168.188.210   worker1-kubeadm-k8s   <none>           <none>
  whoami-deployment-678b64444d-jxft7   1/1     Running   0          5m7s   192.168.188.209   worker1-kubeadm-k8s   <none>           <none>
  whoami-deployment-678b64444d-r829q   1/1     Running   0          5m7s   192.168.116.15    worker2-kubeadm-k8s   <none>           <none>
  【curl 192.168.188.210:8000、curl 192.168.188.209:8000、curl 192.168.116.15:8000】，任意节点验证集群内能访问成功
  
  3)、创建 svc：【kubectl expose deployment whoami-deployment】，给指定的资源创建 service，默认创建的是 ClusterIP 类型的 svc
   文件方式创建：
  【
apiVersion: v1
kind: Service
metadata:
  name: tomcat-service
spec:
  ports:
  - port: 80   
    protocol: TCP
    targetPort: 8080
  selector:
    app: tomcat 
  】
    删除 svc【kubectl delete svc whoami-deployment】
    查看service 【kubect get svc】
  NAME                TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
  kubernetes          ClusterIP   10.96.0.1      <none>        443/TCP    6d
  whoami-deployment   ClusterIP   10.97.250.54   <none>        8000/TCP   3s

  4)、通过 service IP 访问，同时有人负载均衡【curl 10.97.250.54:8000】
  
  5)、查看 service 详情信息，发现有一个Endpoints连接了具体3个Pod【kubectl describe svc whoami-deployment】

  6)、结论：集群内 Service-Cluster IP，可以供集群内访问，对很多 Deployment[Pod]进行负载均衡

5、Pod访问外部服务：知道ip，直接访问就好

6、外部访问集群内部资源pod，Service-NodePort，端口资源匮乏（在每一台宿主机映射一个端口），不建议使用这种方式

  1)、NodePort，也是Service的一种类型，因为外部能够访问到集群的物理机器IP，所以就是在集群中每台物理机器上暴露一个相同的端口，比如32008
  
  2)、运行之前演示Cluster IP 的deployment文件【kubectl apply -f whoami-deployment.yaml】
  
  3)、创建 NodePort 类型的 service(svc)，名称为whoami-deployment【kubectl expose deployment whoami-deployment --type=NodePort】
  NAME                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
  kubernetes          ClusterIP   10.96.0.1       <none>        443/TCP          6d
  whoami-deployment   NodePort    10.96.100.117   <none>        8000:30103/TCP   7s
  
  注意上述的端口 30103，实际上就是暴露在集群中物理机器上的端口
  【lsof -i tcp:30103】，查看监听端口信息
  【netstat -ntlp|grep 30103】
  【http://192.168.3.61:30103】，浏览器验证

7、Ingress ，外部访问集群内部服务，官网：【https://kubernetes.io/docs/concepts/services-networking/ingress/】

  1)、创建文件：【vi my-tomcat.yaml】,并 apply 使之运行起来
  【
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tomcat-deployment
  labels:
    app: tomcat
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tomcat
  template:
    metadata:
      labels:
        app: tomcat
    spec:
      containers:
      - name: tomcat
        image: tomcat
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: tomcat-service
spec:
  ports:
  - port: 80   
    protocol: TCP
    targetPort: 8080
  selector:
    app: tomcat 
  #type: NodePort  
  】
  
  2)、验证：宿主机ip：【curl 192.168.188.213:8080】，svc ip【curl 10.108.205.193】

8、以Deployment方式创建Pod，该Pod为Ingress Nginx Controller，要想让外界访问，可以通过Service的NodePort或者HostPort方式，这里选择HostPort，比如指定worker01运行

  1)、确保nginx-controller运行到w1节点上：【kubectl label node worker1-kubeadm-k8s name=ingress】,在yaml文件中增加配置  kind: Deployment
      hostNetwork: true
      nodeSelector:
        name: ingress
  
  2)、搜索nodeSelector，并且要确保 worker1-kubeadm-k8s 节点上的80和443端口没有被占用，【lsof -i tcp:80】、【lsof -i tcp:443】

  3)、镜像拉取需要较长的时间：【kubectl apply -f mandatory.yaml】
  
  4)、【kubectl get pods -n ingress-nginx -o wide】，验证是否运行成功

  5)、拉取镜像需要时间比较长：
    可以查看里面需要哪些镜像：【cat mandatory.yaml | grep image】,可以先把镜像拉取下来再 apply【docker pull quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.26.1】
	          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.26.1		
    查看创建的进度：
	【kubectl get pods -n ingress-nginx -o wide】,得到 name nginx-ingress-controller-7c66dcdd6c-64fj8
	【kubectl describe pod nginx-ingress-controller-7c66dcdd6c-64fj8 -n ingress-nginx】

  6)、创建Ingress以及定义转发规则：【vi nginx-ingress.yaml】
  【
#ingress
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: nginx-ingress
spec:
  rules:
  ##域名映射
  - host: tomcat.tiger.com
    http:
      paths:
      - path: /
        backend:
          serviceName: tomcat-service
          servicePort: 80
  】
  
  8)、【kubectl apply -f nginx-ingress.yaml】
      【kubectl get ingress】
      【kubectl describe ingress nginx-ingress】
  
  9)、修改win的hosts文件，添加dns解析，运行nginx 节点的ip：【192.168.3.61 tomcat.tiger.com】
  
  10)、浏览器访问：【tomcat.tiger.com】，验证能访问成功

  11)、总结：如果以后想要使用 Ingress 网络，其实只要定义 ingress，service 和 pod 即可，前提是要保证 nginx、ingress、controller 已经配置好了。






kind：表示要新建对象的类型

spec.selector：表示需要管理的Pod的label，这里表示包含app: nginx的label的Pod都会被该RC管理

spec.replicas：表示受此RC管理的Pod需要运行的副本数

spec.template：表示用于定义Pod的模板，比如Pod名称、拥有的label以及Pod中运行的应用等

通过改变RC里Pod模板中的镜像版本，可以实现Pod的升级功能

kubectl apply -f nginx-pod.yaml，此时k8s会在所有可用的Node上，创建3个Pod，并且每个Pod都有一个app: nginx的label，同时每个Pod中都运行了一个nginx容器。

如果某个Pod发生问题，Controller Manager能够及时发现，然后根据RC的定义，创建一个新的Pod

扩缩容：kubectl scale rc nginx --replicas=5
  
  删除pod【kubectl delete pod gz-cea-teacher-ui-deploy-56c97f9f7d-84vx4 --force --grace-period=0 -n default】

  
  