
场景 --》需求 --》解决方案 --》应用 --》原理

====== volatile 关键字的本质 ======

1、读和写发生在不同的线程中的时候，可能会出现：读线程不能及时的读取到其他线程写入的最新的值。这就是所谓的可见性为了实现跨线程写入的内存可见性，必须使用到一些机制来实现。而 volatile 就是这样一种机制。

2、为了提升cpu的使用率：编译器的指令优化，更合理的去利用好 CPU 的高速缓存然后每一种优化，都会带来相应的问题，而这些问题也是导致线程安全性问题的根源

3、引入【高速缓存】(解决CPU与内存的速度矛盾，提升CPU的使用率，同时会带来不同CPU之间缓存一致性问题) --> 主内存。

4、CPU乱序排序-》重排序-》可见性问题
   CPU层面提供了指令-》内存屏障
   内存屏障解决可见性问题，让软件层面在适当的时机加入
   CPU层面提供三种屏障
   1.Store Memory Barrier(写屏障)
   2.Load Memory Barrier(读屏障)
   3.Full Memory Barrier(全屏障) 写和读配合使用

5、 volatile 关键字的代码，这个关键字会生成一个 Lock 的汇编指令，这个指令其实就相当于实现了一种内存屏障


====== JMM ======

1、内存屏障、重排序这些东西好像是和平台以及硬件架构有关系的。作为 Java 语言的特性，一次编写多处运行。我们不应该考虑平台相关的问题，并且这些所谓的内存屏障也不应该让程序员来关心

2、JMM 全称是 Java Memory Model. 什么是 JMM 呢？通过前面的分析发现，导致可见性问题的根本原因是缓存以及重排序。 而 JMM 实际上就是提供了合理的禁用缓存以及禁止重排序的方法。所以它最核心的价值在于解决可见性和有序性

3、JMM 属于语言级别的抽象内存模型，可以简单理解为对硬件模型的抽象，它定义了共享内存中多线程程序读写操作的行为规范：在虚拟机中把共享变量存储到内存以及从内存中取出共享变量的底层实现细节通过这些规则来规范对内存的读写操作从而保证指令的正确性，它解决了 CPU 多级缓存、处理器优化、指令重排序导致的内存访问问题，保证了并发场景下的可见性

4、JMM 抽象模型分为主内存、工作内存；主内存是所有线程共享的，一般是实例对象、静态字段、数组对象等存储在堆内存中的变量。工作内存是每个线程独占的，线程对变量的所有操作都必须在工作内存中进行，不能直接读写主
内存中的变量，线程之间的共享变量值的传递都是基于主内存来完成

5、Java 内存模型底层实现可以简单的认为：通过内存屏障(memory barrier)禁止重排序，即时编译器根据具体的底层体系架构，将这些内存屏障替换成具体的 CPU 指令。对于编译器而言，内存屏障将限制它所能做的重排序优化。而对于处理器而言，内存屏障将会导致缓存的刷新操作。比如，对于 volatile，编译器将在 volatile 字段的读写操作前后各插入一些内存屏障。

6、源代码 -》编译器的重排序 -》 cpu层面的重排序(指令、内存) -》最终执行指令

7、重排序规则，数据依赖规则，as-if-serial,as-if-serial语义把单线程程序保护了起来,执行结果不能变。

8、【orderAccess*.hpp】 文件是实现内存屏障的具体文件，不同的平台有不同的实现文件方法
























-server -Xcomp -XX:+UnlockDiagnosticVMOptions -XX:+PrintAssembly XX:CompileCommand=compileonly,*App.*

-server -Xcomp -XX:+UnlockDiagnosticVMOptions -XX:+PrintAssembly -XX:CompileCommand=compileonly,*App.getInstance